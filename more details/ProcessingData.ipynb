{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Twitter data provided by Professor Li\n",
    "import json\n",
    "with open('/Users/yangtang/Desktop/Project/tweets-170a-winter20-coronavirus.json') as f:\n",
    "    data = json.load(f)\n",
    "#print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline=[['2020-01-22','2020-01-28'],['2020-01-29','2020-02-04'],['2020-02-05','2020-02-11'],\n",
    "          ['2020-02-12','2020-02-18'],['2020-02-19','2020-02-25'],['2020-02-26','2020-03-03'],\n",
    "          ['2020-03-04','2020-03-10'],['2020-03-11','2020-03-17'],['2020-03-18','2020-03-24'],\n",
    "          ['2020-03-25','2020-03-31']]\n",
    "datasetsname = ['Jan_TwentySecond','Jan_TwentyNinth','Feb_Fifth','Feb_Twelfth','Feb_Nineteenth',\n",
    "                'Feb_TwentySixth','Mar_Fourth','Mar_Tenth','Mar_Seventeenth','Mar_TwentyFifth']\n",
    "#datasets.clear()\n",
    "datasets={}\n",
    "for i in datasetsname:\n",
    "    datasets[i]=[]\n",
    "#print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate by timeline\n",
    "for tweet in data:\n",
    "    if tweet['in_reply_to_status']==-1:\n",
    "# Remove all replying tweets\n",
    "        for i in range(len(timeline)):\n",
    "            if timeline[i][0]<=tweet['create_at'][:10]<=timeline[i][1]:\n",
    "                datasets[datasetsname[i]].append(tweet)\n",
    "# for i in datasetsname:\n",
    "#     print(i,len(datasets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan_TwentySecond 5911\n",
      "Jan_TwentyNinth 7143\n",
      "Feb_Fifth 4090\n",
      "Feb_Twelfth 3102\n",
      "Feb_Nineteenth 4802\n",
      "Feb_TwentySixth 27692\n",
      "Mar_Fourth 47015\n",
      "Mar_Tenth 149765\n",
      "Mar_Seventeenth 114211\n",
      "Mar_TwentyFifth 57545\n",
      "421276\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in datasetsname:\n",
    "    j+=len(datasets[i])\n",
    "    print(i,len(datasets[i]))\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/user/status/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_if_contaning_links(datasets:dict,timeline):\n",
    "    l=[]\n",
    "    for i in datasets[timeline]:\n",
    "        print(c)\n",
    "        url=\"https://twitter.com/user/status/\"+str(i[\"id\"])\n",
    "        #print(url)\n",
    "        try:\n",
    "            soup = BeautifulSoup(urlopen(url))\n",
    "            productDivs = soup.findAll('div', attrs={'class' : 'js-tweet-text-container'})[0]\n",
    "            for a in productDivs.find_all('a', href=True):\n",
    "                \n",
    "                # check if the raw tweet contains a link to other website or to another tweet\n",
    "                if a['href'].find(\"http\") == 0:\n",
    "                    #print(\"Found the URL:\", a['href'])\n",
    "                    soup2 = BeautifulSoup(urlopen(a['href']))\n",
    "                    \n",
    "                    string = soup2.title.string\n",
    "                    #print(string)\n",
    "                    url2=a['href']\n",
    "                    \n",
    "                    # 3 situation:\n",
    "                    # 1) Include the tweets that is original and have links\n",
    "                    #print(string.find(\"on Twitter\")==-1)#->valid\n",
    "                    f=string.find(\"on Twitter\")\n",
    "                    if f==-1: #->valid\n",
    "                        #print(\"1. IS ORIGINAL\")\n",
    "                        i['links_contained'] = url2\n",
    "                        l.append(i)\n",
    "                        #print(url2)\n",
    "                        break\n",
    "\n",
    "                    # 2) Include the Retweets the news thread with links inside\n",
    "                    # 3) Include the Retweets that tweeted one have links\n",
    "                    elif f!=-1:\n",
    "                        productDivs = soup.findAll('div', attrs={'class' : \"self-thread-context\"})\n",
    "\n",
    "                        \n",
    "                        # If the link is a thread\n",
    "                        if len(productDivs) != 0:\n",
    "                            #print(\"2. IS THREAD-2\")\n",
    "                            productDivs2 = soup2.findAll('div', attrs={'class' : 'js-tweet-text-container'})[0]\n",
    "                            for b in productDivs2.find_all('a', href=True):\n",
    "                                if b['href'].find('http')!=-1 and BeautifulSoup(urlopen(b['href'])).title.string.find(\"on Twitter\")==-1:\n",
    "                                    #print(b['href'])\n",
    "                                    i['links_contained'] = b['href']\n",
    "                                    l.append(i)\n",
    "                            break\n",
    "                                    \n",
    "                        # If the link is not a thread\n",
    "                        else:\n",
    "                            #print(\"3. NOT THREAD\")\n",
    "                            if string.find(\"http\")!=-1:\n",
    "                                substring = string[string.find(\"http\"):]\n",
    "                                extract_link = substring[:substring.find(\" \")]\n",
    "                                extract_link = extract_link[:extract_link.find(\"...\")]\n",
    "                                i['links_contained'] = extract_link\n",
    "                                l.append(i)\n",
    "                            break\n",
    "            \n",
    "        except:\n",
    "            # The tweet page is not avaliable\n",
    "            #print(\"NOT FOUND\")\n",
    "            pass\n",
    "        #print()\n",
    "    #for i in l:\n",
    "    #    print(i['links_contained'])\n",
    "    #print(len(l))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n"
     ]
    }
   ],
   "source": [
    "datasets['Jan_TwentySecond']=check_if_contaning_links(datasets,'Jan_TwentySecond')\n",
    "print(len(datasets['Jan_TwentySecond']))\n",
    "with open('Jan_TwentySecond', 'w') as json_file:\n",
    "    json.dump(datasets['Jan_TwentySecond'],json_file)\n",
    "# with open('Jan_TwentySecond') as f:\n",
    "#     a = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['Mar_Fourth']=check_if_contaning_links(datasets,'Mar_Fourth')\n",
    "print(len(datasets['Mar_Fourth']))\n",
    "with open('Mar_Fourth', 'w') as json_file:\n",
    "    json.dump(datasets['Mar_Fourth'],json_file)\n",
    "# 47015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['Mar_TwentyFifth']=check_if_contaning_links(datasets,'Mar_TwentyFifth')\n",
    "print(len(datasets['Mar_TwentyFifth']))\n",
    "with open('Mar_TwentyFifth', 'w') as json_file:\n",
    "    json.dump(datasets['Mar_TwentyFifth'],json_file)\n",
    "# 57545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan_TwentySecond 5911\n",
    "# Jan_TwentyNinth 7143\n",
    "# Feb_Fifth 4090\n",
    "# Feb_Twelfth 3102\n",
    "# Feb_Nineteenth 4802\n",
    "# Feb_TwentySixth 27692\n",
    "# Mar_Fourth 47015\n",
    "# Mar_Tenth 149765\n",
    "# Mar_Seventeenth 114211\n",
    "# Mar_TwentyFifth 57545\n",
    "\n",
    "# Jan_TwentySecond 1790*\n",
    "# Jan_TwentyNinth 2482*\n",
    "# Feb_Fifth 1884*\n",
    "# Feb_Twelfth 1441*\n",
    "# Feb_Nineteenth 2191*\n",
    "# Feb_TwentySixth 9369*  \n",
    "# Mar_Fourth 14776*\n",
    "# Mar_Tenth 45106*\n",
    "# Mar_Seventeenth 44647*\n",
    "# Mar_TwentyFifth 24759*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
